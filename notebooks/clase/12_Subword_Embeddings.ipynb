{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJBuKgYggV6W"
      },
      "source": [
        "# FastText\n",
        "\n",
        "A diferencia de Word2Vec, que trabaja a nivel de palabra, FastText trata de capturar la información morfológica de las palabras.\n",
        "\n",
        ">*\"[...] we propose a new approach **based on the skipgram model, where each word is represented as a bag of character n-grams**. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. [...]\"* <br>(Mikolov et al., Enriching Word Vectors with Subword Information, https://arxiv.org/pdf/1607.04606.pdf)\n",
        "\n",
        "De esta manera, una palabra quedará representada por sus n-grams.\n",
        "\n",
        "El tamaño de los n-grams deberá ser definido como hiperparámetro\n",
        "- min_n: valor mínimo de _n_ a considerar\n",
        "- max_n: valor máximo de _n_ a considerar\n",
        "\n",
        "Ejemplo:\n",
        ">*\"Me gusta el procesado del lenguaje natural\"*\n",
        ">* Ejemplo de *skip-gram* pre-procesado con una ventana de contexto de 2 palabras\n",
        ">\n",
        ">$w_{target} =$ \"procesado\" &emsp;$w_{context} =$ [\"gusta\", \"el\", \"del\", \"lenguaje\"] \n",
        ">\n",
        ">     (\"procesado\", \"gusta\")\n",
        ">\n",
        "> Descomoposición de n-grams con min_n=3 and max_n=4:\n",
        ">\n",
        ">\"procesado\" = [\"$<$pr\", \"pro\", ..., \"ado\", \"do$>$\", \"$<$pro\", \"roce\", ..., \"sado\", \"ado$>$\"]\n",
        ">\n",
        ">* De este modo, la similitud será: <br><br>\n",
        ">&emsp;$\\boxed{s(w_{target}, w_{context}) = \\sum_{g \\in G_{w_{target}}}z_{g}^T v_{w_{context}}}$, where $G_{w_{target}}\\subset\\{g_{1}, ..., g_{G}\\}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0qLjFS_gV6a"
      },
      "source": [
        "## Palabras más similares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTzIvoQ-gV6b"
      },
      "outputs": [],
      "source": [
        "def print_sim_words(word, model1, model2):\n",
        "    query = \"Most similar to {}\".format(word) \n",
        "    print(query)\n",
        "    print(\"-\"*len(query))\n",
        "    for (sim1, sim2) in zip(model1.wv.most_similar(word), model2.wv.most_similar(word)):\n",
        "        print(\"{}:{}{:.3f}{}{}:{}{:.3f}\".format(sim1[0],\n",
        "                                               \" \"*(20-len(sim1[0])), \n",
        "                                               sim1[1], \n",
        "                                               \" \"*10, \n",
        "                                               sim2[0],\n",
        "                                               \" \"*(20-len(sim2[0])),\n",
        "                                               sim2[1]))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GYhLbgugV6c"
      },
      "source": [
        "## Importamos las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD5YKchbgV6c"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.models.word2vec import LineSentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX8m7DX1gV6c"
      },
      "source": [
        "## Lectura de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_clean = pd.read_csv('./df_clean_simpsons.csv')"
      ],
      "metadata": {
        "id": "QXn1Q5bgOKPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "sent = [row.split() for row in df_clean['clean']]"
      ],
      "metadata": {
        "id": "d1GDH8oeOd5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzpsA8BpgV6d"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsUJBv_igV6d"
      },
      "outputs": [],
      "source": [
        "sg_params = {\n",
        "    'sg': 1,\n",
        "    'size': 300,\n",
        "    'min_count': 5,\n",
        "    'window': 5,\n",
        "    'hs': 0,\n",
        "    'negative': 20,\n",
        "    'workers': 4,\n",
        "    'min_n': 3,\n",
        "    'max_n': 6\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT5AbNulgV6d"
      },
      "source": [
        "## Inicializamos el objeto FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmC7HKDggV6e",
        "outputId": "e0c0be9f-a87c-4968-bff4-ff9afcd07945",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class FastText in module gensim.models.fasttext:\n",
            "\n",
            "class FastText(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
            " |  FastText(sentences=None, corpus_file=None, sg=0, hs=0, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=())\n",
            " |  \n",
            " |  Train, use and evaluate word representations learned using the method\n",
            " |  described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_, aka FastText.\n",
            " |  \n",
            " |  The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n",
            " |  :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the original\n",
            " |  Fasttext implementation via :meth:`~gensim.models.fasttext.FastText.load_fasttext_format`.\n",
            " |  \n",
            " |  Some important internal attributes are the following:\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  wv : :class:`~gensim.models.keyedvectors.FastTextKeyedVectors`\n",
            " |      This object essentially contains the mapping between words and embeddings. These are similar to the embeddings\n",
            " |      computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also include vectors for n-grams.\n",
            " |      This allows the model to compute embeddings even for **unseen** words (that do not exist in the vocabulary),\n",
            " |      as the aggregate of the n-grams included in the word. After training the model, this attribute can be used\n",
            " |      directly to query those embeddings in various ways. Check the module level docstring from some examples.\n",
            " |  vocabulary : :class:`~gensim.models.fasttext.FastTextVocab`\n",
            " |      This object represents the vocabulary of the model.\n",
            " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
            " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
            " |  trainables : :class:`~gensim.models.fasttext.FastTextTrainables`\n",
            " |      This object represents the inner shallow neural network used to train the embeddings. This is very\n",
            " |      similar to the network of the :class:`~gensim.models.word2vec.Word2Vec` model, but it also trains weights\n",
            " |      for the N-Grams (sequences of more than 1 words). The semantics of the network are almost the same as\n",
            " |      the one used for the :class:`~gensim.models.word2vec.Word2Vec` model.\n",
            " |      You can think of it as a NN with a single projection and hidden layer which we train on the corpus.\n",
            " |      The weights are then used as our embeddings. An important difference however between the two models, is the\n",
            " |      scoring function used to compute the loss. In the case of FastText, this is modified in word to also account\n",
            " |      for the internal structure of words, besides their concurrence counts.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      FastText\n",
            " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
            " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
            " |      gensim.utils.SaveLoad\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __contains__(self, word)\n",
            " |      Deprecated. Use self.wv.__contains__() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`gensim.models.keyedvectors.KeyedVectors.__contains__`\n",
            " |  \n",
            " |  __getitem__(self, words)\n",
            " |      Deprecated. Use self.wv.__getitem__() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`gensim.models.keyedvectors.KeyedVectors.__getitem__`\n",
            " |  \n",
            " |  __init__(self, sentences=None, corpus_file=None, sg=0, hs=0, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=())\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str, optional\n",
            " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
            " |          in some other way.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (or none of them).\n",
            " |      min_count : int, optional\n",
            " |          The model ignores all words with total frequency lower than this.\n",
            " |      size : int, optional\n",
            " |          Dimensionality of the word vectors.\n",
            " |      window : int, optional\n",
            " |          The maximum distance between the current and predicted word within a sentence.\n",
            " |      workers : int, optional\n",
            " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
            " |      alpha : float, optional\n",
            " |          The initial learning rate.\n",
            " |      min_alpha : float, optional\n",
            " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
            " |      sg : {1, 0}, optional\n",
            " |          Training algorithm: skip-gram if `sg=1`, otherwise CBOW.\n",
            " |      hs : {1,0}, optional\n",
            " |          If 1, hierarchical softmax will be used for model training.\n",
            " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
            " |      seed : int, optional\n",
            " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
            " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
            " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
            " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
            " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
            " |      max_vocab_size : int, optional\n",
            " |          Limits the RAM during vocabulary building; if there are more unique\n",
            " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
            " |          Set to `None` for no limit.\n",
            " |      sample : float, optional\n",
            " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
            " |          useful range is (0, 1e-5).\n",
            " |      negative : int, optional\n",
            " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
            " |          should be drawn (usually between 5-20).\n",
            " |          If set to 0, no negative sampling is used.\n",
            " |      ns_exponent : float, optional\n",
            " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
            " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
            " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
            " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
            " |          other values may perform better for recommendation applications.\n",
            " |      cbow_mean : {1,0}, optional\n",
            " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
            " |      hashfxn : function, optional\n",
            " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
            " |      iter : int, optional\n",
            " |          Number of iterations (epochs) over the corpus.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during\n",
            " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of themodel.\n",
            " |      \n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |      \n",
            " |      sorted_vocab : {1,0}, optional\n",
            " |          If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
            " |      batch_words : int, optional\n",
            " |          Target size (in words) for batches of examples passed to worker threads (and\n",
            " |          thus cython routines).(Larger batches will be passed if individual\n",
            " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
            " |      min_n : int, optional\n",
            " |          Minimum length of char n-grams to be used for training word representations.\n",
            " |      max_n : int, optional\n",
            " |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
            " |          lesser than `min_n` to avoid char ngrams being used.\n",
            " |      word_ngrams : {1,0}, optional\n",
            " |          If 1, uses enriches word vectors with subword(n-grams) information.\n",
            " |          If 0, this is equivalent to :class:`~gensim.models.word2vec.Word2Vec`.\n",
            " |      bucket : int, optional\n",
            " |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n",
            " |          memory usage of the model. This option specifies the number of buckets used by the model.\n",
            " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
            " |          List of callbacks that need to be executed/run at specific stages during training.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Initialize and train a `FastText` model::\n",
            " |      \n",
            " |      >>> from gensim.models import FastText\n",
            " |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |      >>>\n",
            " |      >>> model = FastText(sentences, min_count=1)\n",
            " |      >>> say_vector = model['say']  # get vector for word\n",
            " |      >>> of_vector = model['of']  # get vector for out-of-vocab word\n",
            " |  \n",
            " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
            " |  \n",
            " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
            " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
            " |      Each sentence must be a list of unicode strings.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str, optional\n",
            " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (not both of them).\n",
            " |      update : bool\n",
            " |          If true, the new words in `sentences` will be added to model's vocab.\n",
            " |      progress_per : int\n",
            " |          Indicates how many words to process before showing/updating the progress.\n",
            " |      keep_raw_vocab : bool\n",
            " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during\n",
            " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of the model.\n",
            " |      \n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |      \n",
            " |      **kwargs\n",
            " |          Additional key word parameters passed to\n",
            " |          :meth:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel.build_vocab`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Train a model and update vocab for online training\n",
            " |      \n",
            " |      >>> from gensim.models import FastText\n",
            " |      >>> sentences_1 = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |      >>> sentences_2 = [[\"dude\", \"say\", \"wazzup!\"]]\n",
            " |      >>>\n",
            " |      >>> model = FastText(min_count=1)\n",
            " |      >>> model.build_vocab(sentences_1)\n",
            " |      >>> model.train(sentences_1, total_examples=model.corpus_count, epochs=model.iter)\n",
            " |      >>>\n",
            " |      >>> model.build_vocab(sentences_2, update=True)\n",
            " |      >>> model.train(sentences_2, total_examples=model.corpus_count, epochs=model.iter)\n",
            " |  \n",
            " |  clear_sims(self)\n",
            " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
            " |      \n",
            " |      You can recompute them later again using the :meth:`~gensim.models.fasttext.FastText.init_sims` method.\n",
            " |  \n",
            " |  estimate_memory(self, vocab_size=None, report=None)\n",
            " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vocab_size : int, optional\n",
            " |          Number of unique tokens in the vocabulary\n",
            " |      report : dict of (str, int), optional\n",
            " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      dict of (str, int)\n",
            " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
            " |  \n",
            " |  init_sims(self, replace=False)\n",
            " |      Precompute L2-normalized vectors.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      replace : bool\n",
            " |          If True, forget the original vectors and only keep the normalized ones to save RAM.\n",
            " |  \n",
            " |  load_binary_data(self, encoding='utf8')\n",
            " |      Load data from a binary file created by Facebook's native FastText.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      encoding : str, optional\n",
            " |          Specifies the encoding.\n",
            " |  \n",
            " |  save(self, *args, **kwargs)\n",
            " |      Save the Fasttext model. This saved model can be loaded again using\n",
            " |      :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\n",
            " |      and getting vectors for out-of-vocabulary words.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Store the model to this file.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.fasttext.FastText.load`\n",
            " |          Load :class:`~gensim.models.fasttext.FastText` model.\n",
            " |  \n",
            " |  struct_unpack(self, file_handle, fmt)\n",
            " |      Read a single object from an open file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      file_handle : file_like object\n",
            " |          Handle to an open file\n",
            " |      fmt : str\n",
            " |          Byte format in which the structure is saved.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      Tuple of (str)\n",
            " |          Unpacked structure.\n",
            " |  \n",
            " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=(), **kwargs)\n",
            " |      Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
            " |      For FastText, each sentence must be a list of unicode strings.\n",
            " |      \n",
            " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
            " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
            " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
            " |      that was provided to :meth:`~gensim.models.fasttext.FastText.build_vocab` earlier,\n",
            " |      you can simply use `total_examples=self.corpus_count`.\n",
            " |      \n",
            " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
            " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
            " |      where :meth:`~gensim.models.fasttext.FastText.train` is only called once, you can set `epochs=self.iter`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str, optional\n",
            " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          If you use this argument instead of `sentences`, you must provide `total_words` argument as well. Only one\n",
            " |          of `sentences` or `corpus_file` arguments need to be passed (not both of them).\n",
            " |      total_examples : int\n",
            " |          Count of sentences.\n",
            " |      total_words : int\n",
            " |          Count of raw words in sentences.\n",
            " |      epochs : int\n",
            " |          Number of iterations (epochs) over the corpus.\n",
            " |      start_alpha : float, optional\n",
            " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
            " |          for this one call to :meth:`~gensim.models.fasttext.FastText.train`.\n",
            " |          Use only if making multiple calls to :meth:`~gensim.models.fasttext.FastText.train`, when you want to manage\n",
            " |          the alpha learning-rate yourself (not recommended).\n",
            " |      end_alpha : float, optional\n",
            " |          Final learning rate. Drops linearly from `start_alpha`.\n",
            " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to\n",
            " |          :meth:`~gensim.models.fasttext.FastText.train`.\n",
            " |          Use only if making multiple calls to :meth:`~gensim.models.fasttext.FastText.train`, when you want to manage\n",
            " |          the alpha learning-rate yourself (not recommended).\n",
            " |      word_count : int\n",
            " |          Count of words already trained. Set this to 0 for the usual\n",
            " |          case of training on all words in sentences.\n",
            " |      queue_factor : int\n",
            " |          Multiplier for size of queue (number of workers * queue_factor).\n",
            " |      report_delay : float\n",
            " |          Seconds to wait before reporting progress.\n",
            " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
            " |          List of callbacks that need to be executed/run at specific stages during training.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from gensim.models import FastText\n",
            " |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |      >>>\n",
            " |      >>> model = FastText(min_count=1)\n",
            " |      >>> model.build_vocab(sentences)\n",
            " |      >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  load(*args, **kwargs) from builtins.type\n",
            " |      Load a previously saved `FastText` model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to the saved file.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`~gensim.models.fasttext.FastText`\n",
            " |          Loaded model.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.fasttext.FastText.save`\n",
            " |          Save :class:`~gensim.models.fasttext.FastText` model.\n",
            " |  \n",
            " |  load_fasttext_format(model_file, encoding='utf8') from builtins.type\n",
            " |      Load the input-hidden weight matrix from Facebook's native fasttext `.bin` and `.vec` output files.\n",
            " |      \n",
            " |      Notes\n",
            " |      ------\n",
            " |      Due to limitations in the FastText API, you cannot continue training with a model loaded this way.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      model_file : str\n",
            " |          Path to the FastText output files.\n",
            " |          FastText outputs two model files - `/path/to/model.vec` and `/path/to/model.bin`\n",
            " |          Expected value for this example: `/path/to/model` or `/path/to/model.bin`,\n",
            " |          as Gensim requires only `.bin` file to the load entire fastText model.\n",
            " |      encoding : str, optional\n",
            " |          Specifies the file encoding.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class: `~gensim.models.fasttext.FastText`\n",
            " |          The loaded model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  bucket\n",
            " |  \n",
            " |  max_n\n",
            " |  \n",
            " |  min_n\n",
            " |  \n",
            " |  num_ngram_vectors\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  syn0_ngrams_lockf\n",
            " |  \n",
            " |  syn0_vocab_lockf\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Get a human readable representation of the object.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      str\n",
            " |          A human readable string containing the class name, as well as the size of dictionary, number of\n",
            " |          features and starting learning rate used by the object.\n",
            " |  \n",
            " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
            " |      Build vocabulary from a dictionary of word frequencies.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      word_freq : dict of (str, int)\n",
            " |          A mapping from a word in the vocabulary to its frequency count.\n",
            " |      keep_raw_vocab : bool, optional\n",
            " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
            " |      corpus_count : int, optional\n",
            " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
            " |          of the model.\n",
            " |      \n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |      \n",
            " |      update : bool, optional\n",
            " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
            " |  \n",
            " |  doesnt_match(self, words)\n",
            " |      Deprecated, use self.wv.doesnt_match() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
            " |  \n",
            " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
            " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
            " |      \n",
            " |      Refer to the documentation for\n",
            " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
            " |  \n",
            " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
            " |      Deprecated, use self.wv.most_similar() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
            " |  \n",
            " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
            " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
            " |      \n",
            " |      Refer to the documentation for\n",
            " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
            " |  \n",
            " |  n_similarity(self, ws1, ws2)\n",
            " |      Deprecated, use self.wv.n_similarity() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
            " |  \n",
            " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
            " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
            " |  \n",
            " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
            " |      Deprecated, use self.wv.similar_by_word() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
            " |  \n",
            " |  similarity(self, w1, w2)\n",
            " |      Deprecated, use self.wv.similarity() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
            " |  \n",
            " |  wmdistance(self, document1, document2)\n",
            " |      Deprecated, use self.wv.wmdistance() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
            " |  \n",
            " |  cum_table\n",
            " |  \n",
            " |  hashfxn\n",
            " |  \n",
            " |  iter\n",
            " |  \n",
            " |  layer1_size\n",
            " |  \n",
            " |  min_count\n",
            " |  \n",
            " |  sample\n",
            " |  \n",
            " |  syn0_lockf\n",
            " |  \n",
            " |  syn1\n",
            " |  \n",
            " |  syn1neg\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(FastText)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVumppbugV6e"
      },
      "source": [
        "## Construímos el vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkuQXsQdHnWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXG1UfHgV6f"
      },
      "source": [
        "## Entrenamos los pesos de los embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "daERlXPXHqZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F62S0H2gV6f"
      },
      "source": [
        "## Guardamos los modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYpQ9YUMHr1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-79KWIfjgV6f"
      },
      "source": [
        "## Algunos resultados"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Um4FO1kHtWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uivGUy4AgV6g"
      },
      "source": [
        "## Out-of-Vocabulary (OOV) Words "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enFbsjjNgV6g"
      },
      "source": [
        "la cantidad de n-grams creados durante el entrenamiento del FastText hace improbable (que no imposible) que alguna palabra no pueda ser construída como una bolsa de n-grams"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5mnkWd0HwAq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}