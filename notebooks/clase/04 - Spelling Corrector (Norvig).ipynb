{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En el campo del NLP, existen multitud de causas por las cuales pueden aparecer errores en campos de texto. Algunas muy comunes pueden ser:\n",
    "- Typos al escribir\n",
    "- Errores de reconocimiento de un ASR\n",
    "- Errores al transcribir imágenes a texto (OCR)\n",
    "\n",
    "Aunque hoy en día existen modelos complejos basados en Deep Learning (como pueden ser los seq2seq), existen otros mucho menos complejos basados en determinar la similitud (o diferencia) entre strings en cuanto al número de cambios lexico-gráficos necesarios para transformar un string en otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distancia de Edición\n",
    "\n",
    "La distancia de edición es un método para calcular la similitud entre cadenas de caracteres. Aunque veremos más ejemplos en el notebook de **Matching** y sucesivos, se introduce aquí para presentar un algoritmo de corrección ortográfica.\n",
    "\n",
    "Dicha distancia tiene en cuenta el **número mínimo de operaciones** requeridas para transformar una cadena de caracteres en otra. También se utiliza en bioinformática para cuantificar la similitud entre cadenas de ADN.\n",
    "\n",
    "En función de las operaciones que se permitan, existen **distintos tipos de Distancia de Edición**:\n",
    "\n",
    "|                Distancia               \t| Inserción \t| Eliminación \t| Sustitución \t| Transposición \t|\n",
    "|:--------------------------------------:\t|:---------:\t|:-----------:\t|:-----------:\t|:-------------:\t|\n",
    "|               Levenshtein              \t|     Y     \t|      Y      \t|      Y      \t|       -       \t|\n",
    "| Subsecuencia Común <br>más Larga (LCS) \t|     Y     \t|      Y      \t|      -      \t|       -       \t|\n",
    "|                Hamming \\*               \t|     -     \t|      -      \t|      Y      \t|       -       \t|\n",
    "|         Damerau-Levenshtein \\**         \t|     Y     \t|      Y      \t|      -      \t|       Y       \t|\n",
    "|                  Jaro                  \t|     -     \t|      -      \t|      -      \t|       Y       \t|\n",
    "\n",
    "- \\* _Solo sirve para strings de igual longitud_\n",
    "- \\** _Transposición de dos caracteres adyacentes_\n",
    "\n",
    "Además del número de operaciones, también debe definirse la penalización por cada operación. Una posibilidad es la de que cada operación suponga sumar +1 a la distancia entre dos strings.\n",
    "\n",
    "> d('_cena_', '_coma_') = 2\n",
    "\n",
    "Si se define un coste positivo se cumple lo siguiente:\n",
    "- d(a, b) = 0, solo si a=b\n",
    "- d(a, b) > 0, si a$\\neq$b\n",
    "- d(a, b) = d(b, a)\n",
    "\n",
    "Hay que tener en cuenta que una edición no tiene porqué resultar en una palabra real.\n",
    "\n",
    "## Distancia de Levenshtein\n",
    "\n",
    "Operaciones permitidas: inserción, eliminación y sustitución.\n",
    "\n",
    "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/4520f5376b54613a5b0e6c6db46083989f901821>\n",
    "\n",
    "En este artículo lo explican de maravilla: [link](https://medium.com/@ethannam/understanding-the-levenshtein-distance-equation-for-beginners-c4285a5604f0)\n",
    "\n",
    "<img src=https://miro.medium.com/max/1108/1*bEWdxv_FoTQurG9fyS3nSA.jpeg width=300px>\n",
    "\n",
    "## Utilidad\n",
    "\n",
    "Encontrar palabras _cercanas_ entre sí en cuanto a su similitud lexico-gráfica permite, entre otras cosas, corregir pequeños errores o incorrecciones en un texto.\n",
    "\n",
    "Veremos un algoritmo sencillo de _Spelling Correction_ desarrollado por  Peter Norvig. [Link](https://norvig.com/spell-correct.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from utils import load_cinema_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path al directorio donde tenemos los datasets con las reviews\n",
    "datasets_path = '../../datasets'\n",
    "corpus_cine_folder = 'corpusCine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dict = load_cinema_reviews(datasets_path, corpus_cine_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Peter Norvig para Spell Checking\n",
    "\n",
    "Peter Norvig, ex-director de Callidad de búsqueda en Google, escribió este _simple_, pero eficiente, algoritmo para corregir errores ortográficos. Según comenta lo escribió en un vuelo transcontinental para explicar el funcionamiento de un corrector ortográfico _funcional_ de una manera _sencilla_\n",
    "\n",
    "[Link](https://norvig.com/spell-correct.html)\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/b/b8/Peter_Norvig_in_2019.jpg width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo funciona?\n",
    "\n",
    "El objetivo principal es, dada una palabra _w_, un conjunto de candidatos que pudieran ser susceptibles de ser su corrección. A priori no podemos conocer si la palabra _w_ es correcta o incorrecta. Tampoco, de ser incorrecta, conocemos la palabra candidata (de existir) a la que debería ser corregida.\n",
    "\n",
    "De esta manera, se buscará encontrar el candidato correcto (corrección) _c_ que, de todos los posibles candidatos, maximize la probabilidad de que, dada la palabra _w_, sea _c_ la corrección.\n",
    "\n",
    "Esto es:\n",
    "<center>argmax<sub>c $\\in$ candidates </sub> P(c|w)</center>\n",
    "\n",
    "Que por Bayes es equivalente a:\n",
    "<center>argmax<sub>c $\\in$ candidates </sub> P(c)P(w|c)/P(w)</center>\n",
    "\n",
    "Donde P(w) puede eliminarse al ser la misma para cada posible candidato:\n",
    "<center>argmax<sub>c $\\in$ candidates </sub> P(c)P(w|c)</center>\n",
    "\n",
    "Los partes de esta expresión son, por tanto:\n",
    "1. Mecanismo de selección: _argmax_\n",
    "Se elige el candidato que maximiza la probabilidad.\n",
    "2. Modelo de candidatos: _c $\\in$ candidates_\n",
    "Elige los candidatos a tener en cuenta.\n",
    "3. Modelo de lenguaje: _P(c)_\n",
    "La probabilidad de que _c_ apareza como palabra en un texto (probabilidad de ocurrencia)\n",
    "4. Modelo de error: _P(w|c)_\n",
    "La probabilidad de que w fuese escrita cuando realmente se quería escribir c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
