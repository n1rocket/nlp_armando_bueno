{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "Como hemos visto, un NER se encarga de etiquetar palabras en un texto en un conjunto de categorías (nombres de persona, de localización, de entidades de tiempo...). \n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*8LOMipM-fmszClg-AwATkQ.png width=600px>\n",
    "\n",
    "A diferencia de un problema de clasificación de documentos (en el que todo el documento es clasificado en una categoría), aquí cada palabra (o, más correctamente, cada token) es etiquetado secuencialmente.\n",
    "\n",
    "Existen distintas aproximaciones para abordar este tipo de problemas:\n",
    "- **_Clásicas_**: principalmente basados en reglas\n",
    "- **Machine Learning**: se distinguen a su vez dos categorías aquí:\n",
    "    - **Clasificación multi-clase**: se busca clasificar cada token entre un conjunto de categorías sin información del contexto del token\n",
    "    - **CRF**: modelar el texto secuencialmente mediante un grafo probabilístico. En cada token se extraen features que tratan de representar su contexto. [Aquí](https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/) lo explican muy bien.\n",
    "- **Deep learning**: estado del arte (como en casi todas las áreas). Utilizar modelos como las Bi-LSTM permite inferir el contexto de un token de una manera más completa tanto de tokens anteriores a el, como posteriores.\n",
    "\n",
    "**Nosotros vamos a implementar una solución basada en un CRF**.\n",
    "\n",
    "<img src=https://i.imgur.com/ukAr3Uh.jpg width=700px>\n",
    "\n",
    "\n",
    "## Pasos que seguiremos para entrenar un modelo de reconocimiento de entidades\n",
    "\n",
    "Etapa de **entrenamiento**\n",
    "1. Obtener un corpus de entrenamiento\n",
    "2. Etiquetar todos los tokens. Aquellos que no correspondan con una categoría se les asocia el label 'O'\n",
    "3. Definir qué features se extraerán \n",
    "4. Entrenar un modelo de clasificación secuencial para predecir las etiquetas de los tokens\n",
    "\n",
    "Etapa de **testeo**\n",
    "1. Obtener un corpus de test etiquetado\n",
    "2. Lanzar el modelo etiquetando los tokens\n",
    "3. Analizar resultados\n",
    "\n",
    "\n",
    "## IO / IOB Encoding\n",
    "\n",
    "Existen dos maneras principales de etiquetas las etiquetas (IO e IOB).\n",
    "\n",
    "- **IO** (inside-ouside): cada token tendrá una etiqueta, **no tiene en cuenta entidades compuestas por varios tokens**, o chunks, como, por ejemplo, 'Nueva York'.\n",
    "- **IOB** (inside-outside-beginning): cada token tendrá una etiqueta. Si tiene en cuenta chunks. Para codificar el inicio y final de las entidades se incluyen los prefijos 'B-' ('Beginning') e 'I-' ('Inside') para indicar que un token es el inicio de una entidad o pertenece a la misma.\n",
    "\n",
    "En ambos, la etiqueta **'O'** ('Outside') significa que el token no pertenece a ninguna entidad.\n",
    "\n",
    "<img src=https://image.slidesharecdn.com/07lectiener-160215132149/95/ie-named-entity-recognition-ner-36-638.jpg>\n",
    "\n",
    "> Debate: ¿cuál pensáis que arroja mejores resultados?\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "Algunas de las features que podemos extraer son:\n",
    "- **Palabras**\n",
    "    - El token actual e información sobre el mismo (mayúsculas, signos de puntuación, ...)\n",
    "    - Tokens anteriores / posteriores e información sobre ellos\n",
    "    - Substrings (word shapes)\n",
    "- **Información lingüística**\n",
    "    - PoS tags (del token, de los anteriores / consecutivos)\n",
    "- **Otras labels**\n",
    "    - NER labels (del token actual, de los anteriores)\n",
    "\n",
    "\n",
    "## ¡Al lío!\n",
    "\n",
    "Vamos a entrenar nuestro primer modelos de reconocimiento de entidades. Para ello, utilizaremos:\n",
    "- Dataset CoNLL 2002, con PoS Tags\n",
    "- Librería sklearn_crf suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoNLL 2002 Shared Tasks con PoS Tags\n",
    "\n",
    "La CoNLL (Conference on Computational Natural Language Learning) es una conferencia anual organizada por la SIGNLL (ACL's Special Interest Group of Natural Language Processing). Usaremos un corpus con **frases en castellano** con información tanto de los **PoS Tags** como de **entidades etiquetadas**.\n",
    "\n",
    "Links:\n",
    "- https://www.cs.upc.edu/~nlp/tools/nerc/nerc.html\n",
    "- https://www.plantl.gob.es/tecnologias-lenguaje/catalogo-TL/campanas-evaluacion/Paginas/conll-2002.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn_crfsuite\n",
    "\n",
    "Instalación: `pip install sklearn-crfsuite`\n",
    "\n",
    "Links:\n",
    "- Tutorial: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\n",
    "- API reference: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    with io.open(filepath, encoding='latin-1') as f:\n",
    "        idsent = 0\n",
    "        sentences = []\n",
    "        for l in f:\n",
    "            if not len(l.strip()):\n",
    "                sentence = []\n",
    "                idsent += 1\n",
    "            else:\n",
    "                word, pos, ner = l.strip().split(' ')\n",
    "                sentences.append(['sentence#'+str(idsent), word, pos, ner])\n",
    "                \n",
    "        df = pd.DataFrame(sentences)\n",
    "        df.columns = ['Sentence#','word','pos','ner']\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_summary(data):\n",
    "    print(\"Number of sentences: \", len(data.groupby(['Sentence#'])))\n",
    "\n",
    "    words = list(set(data[\"word\"].values))\n",
    "    n_words = len(words)\n",
    "    print(\"Number of words in the dataset: \", n_words)\n",
    "\n",
    "    tags = list(set(data[\"ner\"].values))\n",
    "    print(\"NER Tags:\", tags)\n",
    "    n_tags = len(tags)\n",
    "    print(\"Number of Labels: \", n_tags)\n",
    "\n",
    "    print(\"What the dataset looks like:\")\n",
    "    display(data.head(10))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(data):\n",
    "    sentgroups = data.groupby('Sentence#')\n",
    "\n",
    "    sentences = []\n",
    "    for name, g in sentgroups:\n",
    "        s = []\n",
    "        for row in g.itertuples():\n",
    "            s.append((row.word,row.pos,row.ner))\n",
    "        sentences.append(s)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    tags = list(set(data[\"ner\"].values))\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = '../../datasets/CoNLL2002_NER'\n",
    "corpus_train_file = 'esp.train'\n",
    "corpus_test_file = 'esp.testa'\n",
    "corpus_val_file = 'esp.testb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data(os.path.join(datasets_path, corpus_train_file))\n",
    "df_test = load_data(os.path.join(datasets_path, corpus_test_file))\n",
    "df_val = load_data(os.path.join(datasets_path, corpus_val_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = get_sentences(df_train)\n",
    "sentences_test = get_sentences(df_test)\n",
    "sentences_val = get_sentences(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de features\n",
    "\n",
    "Definimos las features que queremos usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2]\n",
    "    }\n",
    "    \n",
    "    if i > 0:\n",
    "        word_1 = sent[i-1][0]\n",
    "        postag_1 = sent[i-1][1]\n",
    "        \n",
    "        features.update({\n",
    "            '-1:word.lower()': word_1.lower(),\n",
    "            '-1:word.istitle()': word_1.istitle(),\n",
    "            '-1:word.isupper()': word_1.isupper(),\n",
    "            '-1:postag': postag_1,\n",
    "            '-1:postag[:2]': postag_1[:2]\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "    \n",
    "    if i < len(sent)-1:\n",
    "        word_1 = sent[i+1][0]\n",
    "        postag_1 = sent[i+1][1]\n",
    "        \n",
    "        features.update({\n",
    "            '+1:word.lower()': word_1.lower(),\n",
    "            '+1:word.istitle()': word_1.istitle(),\n",
    "            '+1:word.isupper()': word_1.isupper(),\n",
    "            '+1:postag': postag_1,\n",
    "            '+1:postag[:2]': postag_1[:2]\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [word[2] for word in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos para entrenamiento, validación y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación\n",
    "\n",
    "Emplear los datos de validación para _tunnear_ el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados finales\n",
    "\n",
    "Una vez ajustado el modelo, usar el test set para evaluar el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transiciones más probables / improbables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Características estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
