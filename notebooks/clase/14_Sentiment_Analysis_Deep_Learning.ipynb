{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNo8Y9k1TjF9"
      },
      "source": [
        "# 1. Obtención de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "_vWCaykOOBwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "cxvDmvqIOEM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tSeoXf5TmkZ"
      },
      "source": [
        "# 2. Exploración de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgfrUjI-Tif3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6b8d86-8c69-4070-acd5-dcd962abfe54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDb reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='./aclImdb'):\n",
        "    \"\"\"Read IMDb movie reviews from given directory.\n",
        "    \n",
        "    Directory structure expected:\n",
        "    - data/\n",
        "        - train/\n",
        "            - pos/\n",
        "            - neg/\n",
        "        - test/\n",
        "            - pos/\n",
        "            - neg/\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Datos y etiquetas que se devolveran en dict anidades replicando la estructura de directorios\n",
        "    data = {}\n",
        "    labels = {}\n",
        "\n",
        "    # Asumimos 2 subdirectorios: train y test\n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "\n",
        "        # Asumimos 2 subdirectorios por sentimmiento (etiqueta): pos, neg\n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            # Recogemos los ficheros por directorio\n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            # Leemos los datos y les asignamos a su etiqueta\n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    labels[data_type][sentiment].append(sentiment)\n",
        "            \n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "    \n",
        "    # Devolvemos los datos y etiquetas en diccionarios anidados\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "data, labels = read_imdb_data()\n",
        "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "        len(data['train']['pos']), len(data['train']['neg']),\n",
        "        len(data['test']['pos']), len(data['test']['neg'])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"train\"][\"pos\"][0]"
      ],
      "metadata": {
        "id": "eZKS-RAZOK3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"train\"][\"neg\"][0]"
      ],
      "metadata": {
        "id": "jfEfi416ONiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "oVpK5Dr-OP6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "sentiment = 'pos'\n",
        "\n",
        "# Combinamos las reviews para el sentimiento deseado\n",
        "combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
        "\n",
        "# Inicializamos el wordcloud\n",
        "wc = WordCloud(background_color='white', max_words=50,\n",
        "        # Actualizamos las stopwords para incluir palabras comunes del tema\n",
        "        stopwords = STOPWORDS.update(['br','film','movie']))\n",
        "\n",
        "plt.imshow(wc.generate(combined_text))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kS35BN91OS3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "sentiment = 'neg'\n",
        "\n",
        "combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
        "\n",
        "wc = WordCloud(background_color='white', max_words=50,\n",
        "        stopwords = STOPWORDS.update(['br','film','movie']))\n",
        "\n",
        "plt.imshow(wc.generate(combined_text))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZwAIawWOWXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a02WXgrVVSK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3584a98e-92bf-47d5-a177-c05a2266ec86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    # Combinamos los datos de las diferentes etiquetas\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    # Mezclamos los datos con sus etiquetas\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Devolvemos unificado training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test\n",
        "\n",
        "\n",
        "data_train, data_test, labels_train, labels_test = prepare_imdb_data(data)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(data_train), len(data_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahh_Bk8TVnNV"
      },
      "source": [
        "# 4. Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BeautifulSoup to easily remove HTML tags\n",
        "from bs4 import BeautifulSoup \n",
        "\n",
        "# RegEx for removing non-letter characters\n",
        "import re\n",
        "\n",
        "# NLTK library for the remaining steps\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")   # download list of stopwords (only once; need not run it again)\n",
        "from nltk.corpus import stopwords # import stopwords\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "VsNDW-BuOhcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def review_to_words(review):\n",
        "    \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
        "    \n",
        "    # Eliminamos las etiquetas HTML\n",
        "    text = BeautifulSoup(review, \"html5lib\").get_text()\n",
        "    # Convertimos a minúscula y quitamos todo lo que no sea texto o números\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
        "    # Dividimos en tokens por espacios\n",
        "    words = text.split()\n",
        "    # Eliminamos stopwords\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "    # Aplicamos stemming\n",
        "    words = [PorterStemmer().stem(w) for w in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "review_to_words(\"\"\"This is just a <em>test</em>.<br/><br />\n",
        "But if it wasn't a test, it would make for a <b>Great</b> movie review!\"\"\")"
      ],
      "metadata": {
        "id": "m6ba990mOkOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if cache_data is None:\n",
        "        words_train = list(map(review_to_words, data_train))\n",
        "        words_test = list(map(review_to_words, data_test))\n",
        "        \n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test\n",
        "\n",
        "\n",
        "words_train, words_test, labels_train, labels_test = preprocess_data(\n",
        "        data_train, data_test, labels_train, labels_test)\n",
        "\n",
        "print(\"\\n--- Raw review ---\")\n",
        "print(data_train[1])\n",
        "print(\"\\n--- Preprocessed words ---\")\n",
        "print(words_train[1])\n",
        "print(\"\\n--- Label ---\")\n",
        "print(labels_train[1])"
      ],
      "metadata": {
        "id": "jzSvD7b4OoIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import joblib\n",
        "\n",
        "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
        "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
        "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
        "    \n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = joblib.load(f)\n",
        "            print(\"Read features from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if cache_data is None:\n",
        "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
        "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
        "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
        "\n",
        "        features_test = vectorizer.transform(words_test).toarray()\n",
        "                \n",
        "        if cache_file is not None:\n",
        "            vocabulary = vectorizer.vocabulary_\n",
        "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
        "                             vocabulary=vocabulary)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                joblib.dump(cache_data, f)\n",
        "            print(\"Wrote features to cache file:\", cache_file)\n",
        "    else:\n",
        "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
        "                cache_data['features_test'], cache_data['vocabulary'])\n",
        "    \n",
        "    return features_train, features_test, vocabulary\n",
        "\n",
        "features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test)\n",
        "\n",
        "print(\"Vocabulary: {} words\".format(len(vocabulary)))\n",
        "\n",
        "import random\n",
        "print(\"Sample words: {}\".format(random.sample(list(vocabulary.keys()), 8)))\n",
        "\n",
        "print(\"\\n--- Preprocessed words ---\")\n",
        "print(words_train[5])\n",
        "print(\"\\n--- Bag-of-Words features ---\")\n",
        "print(features_train[5])\n",
        "print(\"\\n--- Label ---\")\n",
        "print(labels_train[5])"
      ],
      "metadata": {
        "id": "eNL37H3zOvfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9pqGM_MDbQnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frecuencia de las palabras. Observamos algunos outliers. \n",
        "#Esos datos tienen q ser significativos dado que se ha realizado un preprocesamiento previo\n",
        "plt.plot(features_train[5,:])\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k8AW94neO0d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observamos la ley de Zipf\n",
        "word_freq = features_train.sum(axis=0)\n",
        "\n",
        "sorted_word_freq = np.sort(word_freq)[::-1]\n",
        "\n",
        "plt.plot(sorted_word_freq)\n",
        "plt.gca().set_xscale('log')\n",
        "plt.gca().set_yscale('log')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Number of occurrences')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oX40yeEHPOgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpVHk7N3vdoE"
      },
      "outputs": [],
      "source": [
        "import sklearn.preprocessing as pr\n",
        "\n",
        "features_train = pr.normalize(features_train, axis=1)\n",
        "features_test = pr.normalize(features_test, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENBuwKdRvzch"
      },
      "source": [
        "Debate: ¿Por qué normalizamos los BoW?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7irloKrKwC9u"
      },
      "source": [
        "# 4. Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "n_estimators = 32\n",
        "\n",
        "def classify_gboost(X_train, X_test, y_train, y_test):        \n",
        "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, max_depth=1, random_state=42)\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
        "            clf.__class__.__name__,\n",
        "            clf.score(X_train, y_train),\n",
        "            clf.score(X_test, y_test)))\n",
        "    \n",
        "    return clf\n",
        "\n",
        "\n",
        "clf2 = classify_gboost(features_train, features_test, labels_train, labels_test)"
      ],
      "metadata": {
        "id": "RBmYJu-RPSTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTYamnpnwTRD"
      },
      "source": [
        "# 5. Pasamos a Deep Learning!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "vocabulary_size = 5000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
        "print(\"Loaded dataset with {} training samples, {} test samples\".format(len(X_train), len(X_test)))"
      ],
      "metadata": {
        "id": "nV86rxsyPWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_example, y_train_example), (X_test_example, y_test_example) = imdb.load_data(num_words=vocabulary_size)"
      ],
      "metadata": {
        "id": "6w5LSQByPZyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Review ---\")\n",
        "print(X_train[0])\n",
        "print(\"--- Label ---\")\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "P5JKsm_6Pcbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos ya vienen tokenizados (no son palabras, asignados a un identificador)"
      ],
      "metadata": {
        "id": "yWyxiGVMVDmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cambiamos de número a palabras (metodo de Keras)\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print(\"--- Review (with words) ---\")\n",
        "print([id2word.get(i, \" \") for i in X_train[0]])\n",
        "print(\"--- Label ---\")\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "_15Wm7aoPgEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Media de palabras en la reviews.\n",
        "# RRN necesitamos establcer el tamaño de neruonas \n",
        "# primero hacer el análisis para ver el número medio y máxcimo de palabras en las revies.\n",
        "#si una frase se queda corta, la palabrs restantes se rellenaran de 0. Esto sería negativop\n",
        "max = 0\n",
        "mean = []\n",
        "for example in X_train_example:\n",
        "  length = len(example)\n",
        "  mean.append(len(example))\n",
        "  if length > max:\n",
        "    max = length"
      ],
      "metadata": {
        "id": "a2F8ex8jPjed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(mean) / len(mean)"
      ],
      "metadata": {
        "id": "tDLb7-SQPnFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max"
      ],
      "metadata": {
        "id": "k0av59IkPsrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "max_words = 500\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = pad_sequences(X_test, maxlen=max_words)"
      ],
      "metadata": {
        "id": "GFaSOTuSPu4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, GRUV2, SimpleRNN\n",
        "\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "zsn4GDBZPxy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "JCpAueN5P03U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]  # first batch_size samples\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]  # rest for training\n",
        "\n",
        "model.fit(X_train2, y_train2,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "k2mZ68w8P3Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "lKYtBiG2oBpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file = \"lstm_model.h5\"  # HDF5 file\n",
        "model.save(os.path.join(cache_dir, model_file))"
      ],
      "metadata": {
        "id": "5jGtm7ElQOLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
        "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
      ],
      "metadata": {
        "id": "C_f-VlKxQQ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAscCZtK0c8q"
      },
      "source": [
        "Vamos a ver con GRU:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 32\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model_gru.add(GRUV2(100))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model_gru.summary())"
      ],
      "metadata": {
        "id": "gAsfMrRKQUSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfYp0LxU1xhf"
      },
      "outputs": [],
      "source": [
        "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]  # first batch_size samples\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]  # rest for training\n",
        "\n",
        "model_gru.fit(X_train2, y_train2,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "JxKXZZHBQYbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzNScGZU11sv"
      },
      "outputs": [],
      "source": [
        "model_file = \"gru_model.h5\"  # HDF5 file\n",
        "model_gru.save(os.path.join(cache_dir, model_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LzlI2FO11Pf"
      },
      "outputs": [],
      "source": [
        "scores = model_gru.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
        "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_5Si51_0fEM"
      },
      "source": [
        "Y por último con RNN:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 32\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model_rnn.add(SimpleRNN(100))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model_rnn.summary())"
      ],
      "metadata": {
        "id": "lBQzh_i7QcGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US7sBsoA1-g1"
      },
      "outputs": [],
      "source": [
        "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]  # first batch_size samples\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]  # rest for training\n",
        "\n",
        "model_rnn.fit(X_train2, y_train2,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "pKJ_XLbxQglg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUnazEjo2BOD"
      },
      "outputs": [],
      "source": [
        "model_file = \"rnn_model.h5\"  # HDF5 file\n",
        "model_rnn.save(os.path.join(cache_dir, model_file))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model_rnn.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
        "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
      ],
      "metadata": {
        "id": "v3itUbz3QkR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCd568FE0n8y"
      },
      "source": [
        "Y si quitamos el Embedding?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quunsatuTjOR"
      },
      "source": [
        "# 5. RNN + word2vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbdqedRaT8SR"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import multiprocessing as mp\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        ")\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owDKazhkUGfy"
      },
      "outputs": [],
      "source": [
        "# Parámetros del WORD2VEC\n",
        "W2V_SIZE = 300 # tamaño de vectores\n",
        "W2V_WINDOW = 7 # número de palabras que va a mirar alrededor\n",
        "# 32\n",
        "W2V_EPOCH = 5 # número de epoca\n",
        "W2V_MIN_COUNT = 2 #número mínimo de frecuencia\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 500 # número de secuencias de keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0acrxjhaTpHD"
      },
      "outputs": [],
      "source": [
        "def generate_tokenizer(train_df):\n",
        "  #generamos un identificador único para cada una de las palabras\n",
        "  #creamos el vocabulario (tenemos palabra y número)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_df)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print(f\"Total words: {vocab_size}\")\n",
        "    return tokenizer, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz4CmbiHUeMW"
      },
      "outputs": [],
      "source": [
        "def generate_word2vec(train_df):\n",
        "  #para cada review la dividimos en tokens (palabras )\n",
        "  #para todos los documentos tiene todas las palabras generamos un identificados único para cada una de las palabras\n",
        "  #generamos el modelo, utilizamos la librería gemsin con los parametros definidos previamente, vocabulario y luego entrenamiento\n",
        "  #creamos representacion vecrtoriales (en arrays de 300 elementos)\n",
        "    documents = [_text.split() for _text in train_df.review]\n",
        "    w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "        size=W2V_SIZE,\n",
        "        window=W2V_WINDOW,\n",
        "        min_count=W2V_MIN_COUNT,\n",
        "        workers=mp.cpu_count(),\n",
        "    )\n",
        "    w2v_model.build_vocab(documents)\n",
        "\n",
        "    words = w2v_model.wv.vocab.keys()\n",
        "    vocab_size = len(words)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "    return w2v_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3vey-qvU-iQ"
      },
      "outputs": [],
      "source": [
        "# Generamos la capa de embeddings\n",
        "# El modelo no tiene que hacer el fit de la capa de embeddings, se la damos dada\n",
        "\n",
        "def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
        "  # generamos un capa de embedding como en el ejemplo anterior con LSTM y GRU pero\n",
        "  # en este caso lo hacemos por separado y guardamos el objeto\n",
        "  # Generamos la matriz de embedding, inicializamos a 0 con el tamaño de  W2V y vocabulario\n",
        "  # Para cada palabra del vocabulario generamos la representación vectorial de w2v\n",
        "  # y se la asignamos a la matriz. De este modo, generamos la matrix de equivalencia\n",
        "    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    return Embedding(\n",
        "        vocab_size,\n",
        "        W2V_SIZE,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        # Con trainable le indicamos que no se modificar lo que ya viene dentro de esa capa \n",
        "        trainable=False,\n",
        "    )\n",
        "\n",
        "    # Devuelve la capa de embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axtSnGplWlf9"
      },
      "outputs": [],
      "source": [
        "#cargamos los datos con imdb. Tendremos el vocabularion con los índices \n",
        "(X_train_index, y_train_index), (X_test_index, y_test_index) = imdb.load_data(num_words=vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjy1w727V6di"
      },
      "outputs": [],
      "source": [
        "#convertimos a palabras. Una lista con cada review (un string único)\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "\n",
        "X_train_words = []\n",
        "for index, example in enumerate(X_train_index):\n",
        "  words = \" \".join([id2word.get(i, \" \") for i in X_train_index[index]]).strip()\n",
        "  X_train_words.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN9f0ayYgibY"
      },
      "outputs": [],
      "source": [
        "X_test_words = []\n",
        "for index, example in enumerate(X_test_index):\n",
        "  words = \" \".join([id2word.get(i, \" \") for i in X_test_index[index]]).strip()\n",
        "  X_test_words.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_words[10]"
      ],
      "metadata": {
        "id": "8rtAwRNmQpYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIyyQZrYd9RC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "X_train_words = pd.DataFrame(X_train_words, columns=[\"review\"])\n",
        "X_test_words = pd.DataFrame(X_test_words, columns=[\"review\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_words.head()"
      ],
      "metadata": {
        "id": "_0fcZ9kRQtf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, vocab = generate_tokenizer(X_train_words.review)"
      ],
      "metadata": {
        "id": "d26cJ2kHQv3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = generate_word2vec(X_train_words)"
      ],
      "metadata": {
        "id": "0xfnhbOuQyrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "\n",
        "max_words = 500\n",
        "\n",
        "#Pasamos el pading (500 palabras), tokenizamos pero antes pasamos el texto a secuencias \n",
        "#Cuando tokenizamos pasamos todas las palabras y se convierte a un identificor único del vocabulario \n",
        "\n",
        "X_train_words = pad_sequences(tokenizer.texts_to_sequences(X_train_words.review), maxlen=max_words)\n",
        "X_test_words = pad_sequences(tokenizer.texts_to_sequences(X_test_words.review), maxlen=max_words)"
      ],
      "metadata": {
        "id": "zFd6At1tsXZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyyGw__gYlbu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "embedding_layer = generate_embedding(word2vec_model, vocab, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhwslU8BYvVi"
      },
      "outputs": [],
      "source": [
        "model_custom = Sequential()\n",
        "#pasamos directamente la capa que hemos generado\n",
        "model_custom.add(embedding_layer)\n",
        "#dropout, función entre celdas y el recurrent_dropout añade aleatoriedad (afecta a las gates)\n",
        "model_custom.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_custom.add(Dense(1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_custom.summary()"
      ],
      "metadata": {
        "id": "Nr4uA0OoRMcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWpqnw7jY_FM"
      },
      "outputs": [],
      "source": [
        "model_custom.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "\n",
        "X_train_words_valid, y_valid = X_train_words[:batch_size], y_train[:batch_size]  # first batch_size samples\n",
        "X_train_words2, y_train2 = X_train_words[batch_size:], y_train[batch_size:]  # rest for training\n",
        "\n",
        "model_custom.fit(X_train_words2, y_train2,\n",
        "          validation_data=(X_train_words_valid, y_valid),\n",
        "          batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "fPllrT88RPp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}