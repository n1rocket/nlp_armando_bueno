{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.statmt.org/europarl/v7/fr-en.tgz"
      ],
      "metadata": {
        "id": "2zybRQ28rK1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5166965a-2ed5-4a01-f5f8-e50d85cf9fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-28 19:38:09--  http://www.statmt.org/europarl/v7/fr-en.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.statmt.org/europarl/v7/fr-en.tgz [following]\n",
            "--2023-02-28 19:38:09--  https://www.statmt.org/europarl/v7/fr-en.tgz\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 202718517 (193M) [application/x-gzip]\n",
            "Saving to: ‘fr-en.tgz’\n",
            "\n",
            "fr-en.tgz           100%[===================>] 193.33M  1.50MB/s    in 2m 6s   \n",
            "\n",
            "2023-02-28 19:40:16 (1.54 MB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKi71m3l4FSX",
        "outputId": "051d63ee-c097-4227-a4a5-6e534f82ba68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "europarl-v7.fr-en.en\n",
            "europarl-v7.fr-en.fr\n"
          ]
        }
      ],
      "source": [
        "! tar zxvf fr-en.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo5qx0s34rfK",
        "outputId": "87452d34-85af-48e7-e038-5e38e370a9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumption of the session\n",
            "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
            "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
            "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
            "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
            "Please rise, then, for this minute' s silence.\n",
            "(The House rose and observed a minute' s silence)\n",
            "Madam President, on a point of order.\n",
            "You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
            "One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n"
          ]
        }
      ],
      "source": [
        "! head europarl-v7.fr-en.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRL37p-y4vgF",
        "outputId": "54dc71f5-9caf-4bcf-f14e-72fe3b853c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reprise de la session\n",
            "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
            "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
            "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
            "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
            "Je vous invite à vous lever pour cette minute de silence.\n",
            "(Le Parlement, debout, observe une minute de silence)\n",
            "Madame la Présidente, c'est une motion de procédure.\n",
            "Vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au Sri Lanka.\n",
            "L'une des personnes qui vient d'être assassinée au Sri Lanka est M. Kumar Ponnambalam, qui avait rendu visite au Parlement européen il y a quelques mois à peine.\n"
          ]
        }
      ],
      "source": [
        "! head europarl-v7.fr-en.fr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtYdQTiq44IA"
      },
      "source": [
        "# 1. Carga del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGVBRSz2L2UI",
        "outputId": "bfd6d451-f02e-47f9-f758-27b645432598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.8/dist-packages/tensorflow-2.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled tensorflow-2.11.0\n"
          ]
        }
      ],
      "source": [
        "! pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow-gpu==2.4.0"
      ],
      "metadata": {
        "id": "fgOoOEZKM0rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_jyPKxqD5Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b2a96b-6ab6-411d-b694-205750f1bc4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 17798052016013952355\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ],
      "metadata": {
        "id": "5VLQX4MTM4tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jd7ZKZd4xne"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5fC0IRr5ALr"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3FGx2Xq4_XH",
        "outputId": "b38ff6e3-bda5-4b02-9abf-2bffedaffbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded\n",
            "2007724\n"
          ]
        }
      ],
      "source": [
        "# Load English data\n",
        "english_sentences = load_data('europarl-v7.fr-en.en')\n",
        "# Load French data\n",
        "french_sentences = load_data('europarl-v7.fr-en.fr')\n",
        "\n",
        "print('Dataset Loaded')\n",
        "print(len(english_sentences))\n",
        "\n",
        "dataset_size = int(len(english_sentences) / 6)\n",
        "\n",
        "# Let's use a small dataset\n",
        "english_sentences = english_sentences[:dataset_size]\n",
        "french_sentences = french_sentences[:dataset_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr7koZWF50HS",
        "outputId": "d38470f1-1aee-4598-bc65-a4c9b5edb19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "europarl-v7.fr-en.en Line 1:  Resumption of the session\n",
            "europarl-v7.fr-en.fr Line 1:  Reprise de la session\n",
            "europarl-v7.fr-en.en Line 2:  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
            "europarl-v7.fr-en.fr Line 2:  Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n"
          ]
        }
      ],
      "source": [
        "for sample_i in range(2):\n",
        "    print('europarl-v7.fr-en.en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('europarl-v7.fr-en.fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lka0PDVx583g",
        "outputId": "6b3064a8-9df0-4aa8-d85e-766c330f06ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8506280 English words.\n",
            "116750 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"the\" \"of\" \"to\" \"and\" \"in\" \"a\" \"is\" \"that\" \"I\" \"for\"\n",
            "\n",
            "8917027 French words.\n",
            "145362 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"de\" \"la\" \"et\" \"le\" \"à\" \"des\" \"les\" \"que\" \"en\" \"du\"\n"
          ]
        }
      ],
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSd-D1sv6mL1"
      },
      "source": [
        "# 2. Preprocesado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbZlfpWB6Hge",
        "outputId": "15d5acdf-3b8c-472d-8de2-f1a4d87d5856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ]
        }
      ],
      "source": [
        "def tokenize(x):\n",
        "    \"\"\"\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "    \"\"\"\n",
        "    t = Tokenizer()\n",
        "    # fit the tokenizer on the documents\n",
        "    t.fit_on_texts(x)\n",
        "    return t.texts_to_sequences(x), t\n",
        "\n",
        "\n",
        "# Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx2CfN6o6kcS",
        "outputId": "80cdbdfb-047a-4bb6-8412-4757c461fcbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 4 5 6 7 1 8 9]\n",
            "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
            "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "  Input:  [18 19  3 20 21]\n",
            "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ]
        }
      ],
      "source": [
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen=length, padding='post')\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpDG_eGg7kqX",
        "outputId": "d9a72cfb-3304-4f8b-ebc2-ce9ebf133021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 260\n",
            "Max French sentence length: 224\n",
            "English vocabulary size: 48635\n",
            "French vocabulary size: 68648\n"
          ]
        }
      ],
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "    Preprocess x and y\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)\n",
        "\n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaBUhGze8L9p"
      },
      "source": [
        "# 3. Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEI6J_hM8T4Z"
      },
      "source": [
        "Primero usaremos una implementación de RNN many to many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTzMJKW8-Fs1",
        "outputId": "1dca7025-b8ef-484b-d9c7-7fb188257822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`logits_to_text` function loaded.\n"
          ]
        }
      ],
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    \"\"\"\n",
        "    Turn logits from a neural network into text using the tokenizer\n",
        "    :param logits: Logits from a neural network\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1).astype('float32')])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg6ZrEV5eK03"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "for device in gpu_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "learning_rate = 1e-3\n",
        "\n",
        "input_seq = tf.keras.layers.Input(tmp_x.shape[1:])\n",
        "rnn = tf.compat.v1.keras.layers.CuDNNLSTM(100, return_sequences=True)(input_seq)\n",
        "# french_vocab_size\n",
        "logits = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(french_vocab_size))(rnn)\n",
        "\n",
        "model = tf.keras.models.Model(input_seq, tf.keras.layers.Activation('softmax')(logits))\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(tmp_x, preproc_french_sentences, batch_size=64, epochs=1, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "\n",
        "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "metadata": {
        "id": "cIm4xqbtNALS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6CUKLJmsQInh",
        "outputId": "96271727-6c01-4091-edc0-6fd11d3d1417"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "english_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
        "y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "sentence = 'he saw a old yellow truck'\n",
        "sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
        "sentence = pad_sequences([sentence], maxlen=tmp_x.shape[-1], padding='post')\n",
        "sentence = np.asarray(sentence).astype('float32')\n",
        "#sentences = np.array([sentence[0], tmp_x[0]])\n",
        "predictions = model.predict(sentences, len(sentences))\n",
        "# print(logits_to_text(predictions, french_tokenizer))\n",
        "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))"
      ],
      "metadata": {
        "id": "2EKFT_d_zVh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits_to_text(model.predict(tmp_x[:])[0], french_tokenizer))"
      ],
      "metadata": {
        "id": "Ji980pUPzYzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvZ0LWe7ja9o"
      },
      "source": [
        "Añadimos una capa de Embedding previa a la capa de GRU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo9z_CaaA53R"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "input_seq = tf.keras.layers.Input(tmp_x.shape[2:])\n",
        "embedding = tf.keras.layers.Embedding(input_dim=english_vocab_size, output_dim=256)(input_seq)\n",
        "rnn = tf.keras.layers.GRU(100, return_sequences=True)(embedding)\n",
        "# french_vocab_size\n",
        "logits = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(french_vocab_size))(rnn)\n",
        "\n",
        "model = tf.keras.models.Model(input_seq, tf.keras.layers.Activation('softmax')(logits))\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(tmp_x, preproc_french_sentences, batch_size=64, epochs=1, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r77GuekkGyd"
      },
      "source": [
        "Ahora usamos LSTM Bidireccionales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJcpt5T_T_-f"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "input_seq = Input(tmp_x.shape[1:])\n",
        "rnn = Bidirectional(GRU(100, return_sequences=True))(input_seq)\n",
        "logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
        "\n",
        "model = Model(input_seq, Activation('softmax')(logits))\n",
        "model.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(tmp_x, preproc_french_sentences, batch_size=64, epochs=1, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqTWt0Ckogn"
      },
      "source": [
        "Ahora vamos a implementar un traductor basado en seq2seq con Encoders y Decoders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnwb4e8xk6H5",
        "outputId": "5c6c4ff8-d8cf-4271-f60c-d1894cb09258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3599/8366 [===========>..................] - ETA: 1:37:25 - loss: 1.1479 - accuracy: 0.8824"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "learning_rate = 1e-3\n",
        "    \n",
        "#Encoder\n",
        "encoder_input_seq = Input(shape=tmp_x.shape[1:])\n",
        "encoder_output, state_h, state_c = LSTM(units=400, \n",
        "                              return_sequences=False,\n",
        "                              return_state=True)(encoder_input_seq)\n",
        "    \n",
        "#Decoder  \n",
        "decoder_input_seq = RepeatVector(max_french_sequence_length)(encoder_output)\n",
        "decoder_out = LSTM(units=400,\n",
        "                  return_sequences=True,\n",
        "                  return_state=False)(decoder_input_seq, initial_state=[state_h, state_c])\n",
        "logits = TimeDistributed(Dense(units=french_vocab_size))(decoder_out) \n",
        "\n",
        "#Model\n",
        "model = Model(encoder_input_seq, Activation('softmax')(logits))\n",
        "model.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(lr=learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(tmp_x, preproc_french_sentences, batch_size=32, epochs=1, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}