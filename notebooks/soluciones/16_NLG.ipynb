{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! unzip merged_clean.txt.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ypsdi4S1SGw",
        "outputId": "9154a16f-20d1-4b43-ec95-8fe70085f1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  merged_clean.txt.zip\n",
            "replace merged_clean.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: merged_clean.txt        \n",
            "  inflating: __MACOSX/._merged_clean.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vS_hODoAgdj",
        "outputId": "5b2fd320-26b9-4ec4-d29f-0fa559fb8bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.2.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (217 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m217.1/217.1 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.8/dist-packages (from hmmlearn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.8/dist-packages (from hmmlearn) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.8/dist-packages (from hmmlearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.16->hmmlearn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! head merged_clean.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIn9DtZEQyyy",
        "outputId": "5812a841-4c18-4a50-bf53-458d09d5a5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lovely Ilonka\n",
            "\n",
            "There was once a king's son who told his father that he wished to marry.\n",
            "\n",
            "'No, no!' said the king; 'you must not be in such a hurry. Wait till you\n",
            "have done some great deed. My father did not let me marry till I had won\n",
            "the golden sword you see me wear.'\n",
            "\n",
            "The prince was much disappointed, but he never dreamed of disobeying his\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Model"
      ],
      "metadata": {
        "id": "67pzcYDr38D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, json, codecs, pickle, argparse\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from hmmlearn import hmm\n",
        "from nltk import FreqDist"
      ],
      "metadata": {
        "id": "iATIPKrkwnk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"./merged_clean.txt\", 'r', encoding='utf-8').read()\n",
        "sentences = file.split(\"\\n\")\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "for index, sentence in enumerate(sentences):\n",
        "  if sentence == \"\":\n",
        "      del sentences[index]"
      ],
      "metadata": {
        "id": "ebq9VbohwvWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [line.split() for line in sentences]\n",
        "\n",
        "for line_index, line in enumerate(lines):\n",
        "  for word_index, word in enumerate(line):\n",
        "    lines[line_index][word_index] = word.translate(table)\n",
        "\n",
        "words = [word.lower() for line in lines for word in line]\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "for index, word in enumerate(words):\n",
        "    words[index] = word.translate(table)\n",
        "\n",
        "alphabet = set(words)\n",
        "le = LabelEncoder()\n",
        "le.fit(list(alphabet))\n",
        "\n",
        "seq = le.transform(words)\n",
        "features = np.fromiter(seq, np.int64)\n",
        "features = features.reshape(-1, 1)\n",
        "# features = np.atleast_2d(features).T\n",
        "fd = FreqDist(seq)"
      ],
      "metadata": {
        "id": "reXU0xm2wbD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(alphabet))\n",
        "print(words[:10])\n",
        "print(seq[:10])\n",
        "print(features[:10])\n",
        "print(fd.get(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3apirTiQ8dG",
        "outputId": "e83cb330-dc63-4b44-898a-d1450ad88e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39738\n",
            "['lovely', 'ilonka', 'there', 'was', 'once', 'a', 'kings', 'son', 'who', 'told']\n",
            "[21341 17993 35093 38019 24826   574 19881 32604 38667 35713]\n",
            "[[21341]\n",
            " [17993]\n",
            " [35093]\n",
            " [38019]\n",
            " [24826]\n",
            " [  574]\n",
            " [19881]\n",
            " [32604]\n",
            " [38667]\n",
            " [35713]]\n",
            "225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = hmm.CategoricalHMM(n_components=3, init_params=\"ste\")"
      ],
      "metadata": {
        "id": "XNoht_gpwe12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lines[:2])\n",
        "print(features[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn64Y6cAyHg3",
        "outputId": "d3682fd5-80ae-4a54-c5c4-76e766120866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Lovely', 'Ilonka'], ['There', 'was', 'once', 'a', 'kings', 'son', 'who', 'told', 'his', 'father', 'that', 'he', 'wished', 'to', 'marry']]\n",
            "[[21341]\n",
            " [17993]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.fit(features)"
      ],
      "metadata": {
        "id": "8FBKxbdNyB8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symbols, _states = model.sample(20, random_state=42)\n",
        "output = le.inverse_transform(np.squeeze(symbols))\n",
        "for word in output:\n",
        "        print(word, end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkQnth0W1uJz",
        "outputId": "c5ba5958-625c-4fc3-cef1-79fa3abefb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "who never carry to stable with earrings country of get before his the my and come while that and know "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2"
      ],
      "metadata": {
        "id": "69_SHtgT_Inv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --force-reinstall transformers[tf-cpu] tensorflow==2.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b77HDklG_Jwk",
        "outputId": "b2a26e80-1e1c-43dd-9be9-95ff1939c83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[tf-cpu]\n",
            "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4.0\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Using cached h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Using cached grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-67.4.0-py3-none-any.whl (1.1 MB)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Using cached wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting numpy>=1.20\n",
            "  Using cached numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting protobuf>=3.9.2\n",
            "  Using cached protobuf-4.22.0-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "Collecting six>=1.12.0\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libclang>=9.0.1\n",
            "  Using cached libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
            "Collecting gast>=0.2.1\n",
            "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Collecting flatbuffers>=1.12\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Using cached regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Using cached tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "Collecting tqdm>=4.27\n",
            "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "Collecting tf2onnx\n",
            "  Using cached tf2onnx-1.13.0-py3-none-any.whl (442 kB)\n",
            "Collecting tensorflow-text\n",
            "  Using cached tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "Collecting tensorflow-cpu<2.12,>=2.4\n",
            "  Using cached tensorflow_cpu-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (221.4 MB)\n",
            "Collecting keras-nlp>=0.3.1\n",
            "  Using cached keras_nlp-0.4.1-py3-none-any.whl (466 kB)\n",
            "Collecting onnxconverter-common\n",
            "  Using cached onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
            "Collecting wheel<1.0,>=0.23.0\n",
            "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Using cached google_auth-2.16.1-py2.py3-none-any.whl (177 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "Collecting gast>=0.2.1\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting protobuf>=3.9.2\n",
            "  Using cached protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Collecting tensorflow-cpu<2.12,>=2.4\n",
            "  Using cached tensorflow_cpu-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214.5 MB)\n",
            "  Using cached tensorflow_cpu-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214.4 MB)\n",
            "  Using cached tensorflow_cpu-2.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207.5 MB)\n",
            "  Using cached tensorflow_cpu-2.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207.5 MB)\n",
            "  Using cached tensorflow_cpu-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207.4 MB)\n",
            "  Using cached tensorflow_cpu-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207.4 MB)\n",
            "  Using cached tensorflow_cpu-2.8.4-cp38-cp38-manylinux2010_x86_64.whl (191.4 MB)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx\n",
            "  Using cached onnx-1.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Collecting tensorflow-text\n",
            "  Using cached tensorflow_text-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "  Using cached tensorflow_text-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "  Using cached tensorflow_text-2.8.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "Collecting tensorflow-hub>=0.8.0\n",
            "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
            "Collecting flatbuffers>=1.12\n",
            "  Using cached flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Using cached importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
            "Collecting onnx\n",
            "  Using cached onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "  Using cached onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Using cached MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Installing collected packages: tokenizers, tf-estimator-nightly, tensorflow-estimator, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, charset-normalizer, zipp, wrapt, wheel, urllib3, typing-extensions, tqdm, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, six, setuptools, rsa, regex, pyyaml, pyasn1-modules, protobuf, packaging, oauthlib, numpy, MarkupSafe, idna, grpcio, gast, filelock, certifi, cachetools, absl-py, werkzeug, tensorflow-hub, requests, opt-einsum, onnx, keras-preprocessing, importlib-metadata, h5py, google-pasta, google-auth, astunparse, tf2onnx, requests-oauthlib, onnxconverter-common, markdown, huggingface-hub, transformers, google-auth-oauthlib, tensorboard, tensorflow-cpu, tensorflow, tensorflow-text, keras-nlp\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.2\n",
            "    Uninstalling tokenizers-0.13.2:\n",
            "      Successfully uninstalled tokenizers-0.13.2\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard-plugin-wit\n",
            "    Found existing installation: tensorboard-plugin-wit 1.8.1\n",
            "    Uninstalling tensorboard-plugin-wit-1.8.1:\n",
            "      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.4.8\n",
            "    Uninstalling pyasn1-0.4.8:\n",
            "      Successfully uninstalled pyasn1-0.4.8\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 15.0.6.1\n",
            "    Uninstalling libclang-15.0.6.1:\n",
            "      Successfully uninstalled libclang-15.0.6.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0.7\n",
            "    Uninstalling flatbuffers-2.0.7:\n",
            "      Successfully uninstalled flatbuffers-2.0.7\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.0.1\n",
            "    Uninstalling charset-normalizer-3.0.1:\n",
            "      Successfully uninstalled charset-normalizer-3.0.1\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.15.0\n",
            "    Uninstalling zipp-3.15.0:\n",
            "      Successfully uninstalled zipp-3.15.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.38.4\n",
            "    Uninstalling wheel-0.38.4:\n",
            "      Successfully uninstalled wheel-0.38.4\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.14\n",
            "    Uninstalling urllib3-1.26.14:\n",
            "      Successfully uninstalled urllib3-1.26.14\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.2.0\n",
            "    Uninstalling termcolor-2.2.0:\n",
            "      Successfully uninstalled termcolor-2.2.0\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.31.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.31.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.31.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.4.0\n",
            "    Uninstalling setuptools-67.4.0:\n",
            "      Successfully uninstalled setuptools-67.4.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.10.31\n",
            "    Uninstalling regex-2022.10.31:\n",
            "      Successfully uninstalled regex-2022.10.31\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1-modules 0.2.8\n",
            "    Uninstalling pyasn1-modules-0.2.8:\n",
            "      Successfully uninstalled pyasn1-modules-0.2.8\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.2\n",
            "    Uninstalling oauthlib-3.2.2:\n",
            "      Successfully uninstalled oauthlib-3.2.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.2\n",
            "    Uninstalling numpy-1.24.2:\n",
            "      Successfully uninstalled numpy-1.24.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.2\n",
            "    Uninstalling MarkupSafe-2.1.2:\n",
            "      Successfully uninstalled MarkupSafe-2.1.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.51.3\n",
            "    Uninstalling grpcio-1.51.3:\n",
            "      Successfully uninstalled grpcio-1.51.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.9.0\n",
            "    Uninstalling filelock-3.9.0:\n",
            "      Successfully uninstalled filelock-3.9.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.12.7\n",
            "    Uninstalling certifi-2022.12.7:\n",
            "      Successfully uninstalled certifi-2022.12.7\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 2.2.3\n",
            "    Uninstalling Werkzeug-2.2.3:\n",
            "      Successfully uninstalled Werkzeug-2.2.3\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.12.0\n",
            "    Uninstalling tensorflow-hub-0.12.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.12.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.2\n",
            "    Uninstalling requests-2.28.2:\n",
            "      Successfully uninstalled requests-2.28.2\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Attempting uninstall: onnx\n",
            "    Found existing installation: onnx 1.12.0\n",
            "    Uninstalling onnx-1.12.0:\n",
            "      Successfully uninstalled onnx-1.12.0\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.8.0\n",
            "    Uninstalling h5py-3.8.0:\n",
            "      Successfully uninstalled h5py-3.8.0\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.16.1\n",
            "    Uninstalling google-auth-2.16.1:\n",
            "      Successfully uninstalled google-auth-2.16.1\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Attempting uninstall: tf2onnx\n",
            "    Found existing installation: tf2onnx 1.13.0\n",
            "    Uninstalling tf2onnx-1.13.0:\n",
            "      Successfully uninstalled tf2onnx-1.13.0\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 1.3.1\n",
            "    Uninstalling requests-oauthlib-1.3.1:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.1\n",
            "  Attempting uninstall: onnxconverter-common\n",
            "    Found existing installation: onnxconverter-common 1.13.0\n",
            "    Uninstalling onnxconverter-common-1.13.0:\n",
            "      Successfully uninstalled onnxconverter-common-1.13.0\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.4.1\n",
            "    Uninstalling Markdown-3.4.1:\n",
            "      Successfully uninstalled Markdown-3.4.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.12.1\n",
            "    Uninstalling huggingface-hub-0.12.1:\n",
            "      Successfully uninstalled huggingface-hub-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.26.1\n",
            "    Uninstalling transformers-4.26.1:\n",
            "      Successfully uninstalled transformers-4.26.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow-cpu\n",
            "    Found existing installation: tensorflow-cpu 2.7.4\n",
            "    Uninstalling tensorflow-cpu-2.7.4:\n",
            "      Successfully uninstalled tensorflow-cpu-2.7.4\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: tensorflow-text\n",
            "    Found existing installation: tensorflow-text 2.7.3\n",
            "    Uninstalling tensorflow-text-2.7.3:\n",
            "      Successfully uninstalled tensorflow-text-2.7.3\n",
            "  Attempting uninstall: keras-nlp\n",
            "    Found existing installation: keras-nlp 0.4.1\n",
            "    Uninstalling keras-nlp-0.4.1:\n",
            "      Successfully uninstalled keras-nlp-0.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.2 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-3.0.1 filelock-3.9.0 flatbuffers-2.0.7 gast-0.5.3 google-auth-2.16.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-3.8.0 huggingface-hub-0.12.1 idna-3.4 importlib-metadata-6.0.0 keras-2.8.0 keras-nlp-0.4.1 keras-preprocessing-1.1.2 libclang-15.0.6.1 markdown-3.4.1 numpy-1.24.2 oauthlib-3.2.2 onnx-1.12.0 onnxconverter-common-1.13.0 opt-einsum-3.3.0 packaging-23.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyyaml-6.0 regex-2022.10.31 requests-2.28.2 requests-oauthlib-1.3.1 rsa-4.9 setuptools-67.4.0 six-1.16.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-cpu-2.8.4 tensorflow-estimator-2.8.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.31.0 tensorflow-text-2.8.2 termcolor-2.2.0 tf-estimator-nightly-2.8.0.dev2021122109 tf2onnx-1.13.0 tokenizers-0.13.2 tqdm-4.64.1 transformers-4.26.1 typing-extensions-4.5.0 urllib3-1.26.14 werkzeug-2.2.3 wheel-0.38.4 wrapt-1.15.0 zipp-3.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TextDataset,\n",
        "    GPT2LMHeadModel,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline)"
      ],
      "metadata": {
        "id": "-aGt7kWP_OCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhggQXmY_QKw",
        "outputId": "fd4a4c5e-a9af-41c8-f4a7-9549b738b8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('vocabulary size: %d, max squence length: %d' % (tokenizer.vocab_size, tokenizer.model_max_length))\n",
        "print('tokenize sequence \"Once upon a time in a little village\":', tokenizer('Once upon a time in a little village'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFbiUUjL_Skx",
        "outputId": "890b0e38-3f8a-4e73-9176-fdd1e9b28ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size: 50257, max squence length: 1024\n",
            "tokenize sequence \"Once upon a time in a little village\": {'input_ids': [7454, 2402, 257, 640, 287, 257, 1310, 7404], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "w92Lslq__XcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"merged_clean.txt\",\n",
        "    block_size=128)\n",
        "     \n",
        "test_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"merged_clean.txt\",\n",
        "    block_size=128)"
      ],
      "metadata": {
        "id": "Cz7NDiic_cm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa97a898-0da0-4f9f-984a-a747802ac789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "Loading features from cached file cached_lm_GPT2Tokenizer_128_merged_clean.txt [took 0.003 s]\n",
            "Loading features from cached file cached_lm_GPT2Tokenizer_128_merged_clean.txt [took 0.002 s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(train_dataset[5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMyW28Ap_em4",
        "outputId": "f66b0cd4-75e8-4c17-fd89-febcac56b920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " questioned the magpies the crippled one was the only one who knew where the three bulrushes were.\n",
            "\n",
            "\n",
            "Then the prince started off with the lame magpie. They went on and on till they reached a great stone wall, many, many feet high.\n",
            "\n",
            "\n",
            "‚ÄòNow, prince,‚Äô said the magpie, ‚Äòthe three bulrushes are behind that wall.‚Äô\n",
            "\n",
            "\n",
            "The prince wasted no time. He set his horse at the wall and leaped over it. Then he looked about for the three bulrushes, pulled them up and set off with them on his way home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtWAYmqG_gt0",
        "outputId": "7a02a32d-5a6b-4489-db7b-7351866b70a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = 'data/out', # the output directory for the model predictions and checkpoints\n",
        "    overwrite_output_dir = True, # overwrite the content of the output directory\n",
        "    per_device_train_batch_size = 4, # the batch size for training\n",
        "    per_device_eval_batch_size = 4, # the batch size for evaluation\n",
        "    learning_rate = 5e-5, # defaults to 5e-5\n",
        "    num_train_epochs = 1, # total number of training epochs to perform\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "6agAqbXI_jNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74dd6d9-9d33-4fd1-a9b1-991436833f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "pY44TOgcg8Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "Bh2fp7-7_k4G",
        "outputId": "4b62ad2d-7940-49c8-fa1e-30353972c5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 629\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 158\n",
            "  Number of trainable parameters = 124439808\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [158/158 00:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=158, training_loss=3.22121130062055, metrics={'train_runtime': 36.7198, 'train_samples_per_second': 17.13, 'train_steps_per_second': 4.303, 'total_flos': 41088172032000.0, 'train_loss': 3.22121130062055, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(3600)"
      ],
      "metadata": {
        "id": "XpMs-KAxuzRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_raK2K1_m6F",
        "outputId": "25a58bfc-b483-4212-f8e4-e65cd0fdf0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to data/out\n",
            "Configuration saved in data/out/config.json\n",
            "Configuration saved in data/out/generation_config.json\n",
            "Model weights saved in data/out/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', tokenizer='gpt2', model='data/out')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLzZ1plo_qk_",
        "outputId": "8d58b168-a33f-40c1-fa09-f799e78d942a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file data/out/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"data/out\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file data/out/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"data/out\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file data/out/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at data/out.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading configuration file data/out/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator('There is a building', max_length=500)[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j4fHpMl_sCM",
        "outputId": "e50e5453-c25c-4740-8444-21c8dfa487ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"max_length\": 50,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is a building that is as beautiful as heaven and as shining as every sky and a shining throne that has ever been built in this earth, and it stands for the first time, but to tell you of its importance would require a thousand words. For they say, that there is a great city in the midst of which, that is and still is a very strange city, or rather a country which, having fallen into the hands of a prince and his army, is still in the midst of which there stands the greatest kingdom ever seen.\n",
            "\n",
            "\n",
            "‚ÄòWhere?‚Äô said the little ship. ‚ÄòHave you got some money that I am to give, or something of your own, for the bride?‚Äô\n",
            "\n",
            "\n",
            "‚ÄòHave you, too, and what is your name?‚Äô replied the little ship, ‚Äòbut I would like it out here in peace.‚Äô\n",
            "\n",
            "\n",
            "‚ÄòVery well, then‚Äô answered Mme.‚Äôs father, ‚Äòyou shall be the most valuable man in all the world.‚Äô\n",
            "\n",
            "\n",
            "‚ÄòI will follow my father out into the country, and tell all you need hear about it first,‚Äô answered Mme. ‚Äòfor, now that we have a prince, we must be careful of what he said to us, and in this way shall have to make up for our mistake.‚Äô ‚ÄòNow let us have a little chat.‚Äô\n",
            "\n",
            "\n",
            "Mme. went to the prince‚Äôs room and put some water in it. The prince listened to the water, and asked again about the city.\n",
            "\n",
            "\n",
            "‚ÄòMy father says he does not know of it,‚Äô replied he, ‚Äòso the people don't want to know of it, but the prince is quite sure it is just a lie. Let us say to him: What do you want?‚Äô\n",
            "\n",
            "\n",
            "‚ÄòWell, what you want is a piece of wood, and a gold dagger, and some money which you can use either to buy a crown or go on a pilgrimage to the top of Mount Olympus. You will get it soon, not far apart, and you are only a short distance from the king‚Äôs chamber. But you would want to pay about the country at once so that you might see where the money is.‚Äô\n",
            "\n",
            "\n",
            "‚ÄòI want to go to Mount Olympus on my own free will,‚Äô said\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepESP/gpt2-spanish\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"DeepESP/gpt2-spanish\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKOfdf9FsN5C",
        "outputId": "ca88e1c3-63b4-4a18-92d0-d66007216807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/merges.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at DeepESP/gpt2-spanish.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', tokenizer='DeepESP/gpt2-spanish', model='DeepESP/gpt2-spanish')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYfvRUC8t8Lr",
        "outputId": "6619bb9e-b0d3-44c8-f494-f75d5ed3d382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at DeepESP/gpt2-spanish.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/merges.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--DeepESP--gpt2-spanish/snapshots/1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"DeepESP/gpt2-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator('Hab√≠a una vez', max_length=100)[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlWMrrQRuLjx",
        "outputId": "844d79ac-b322-44ae-ec2a-35a497d2e02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"max_length\": 50,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hab√≠a una vez que el m√©dico lo hab√≠a encontrado, pero se hab√≠a quedado en la cocina, para no verter demasiado. \n",
            "\n",
            "‚ÄîGracias. ‚ÄîAgnes sonri√≥ y vio que las gotas de lluvia no se ca√≠an sobre su rostro. \n",
            "\n",
            "‚ÄîPor cierto, ¬øcu√°l es el problema? \n",
            "\n",
            "‚ÄîEs mi amigo el doctor Simon ‚Äîexplic√≥ √©l, aunque el nombre se le hel√≥ en la boca. \n",
            "\n",
            "‚Äî¬øEl que le ha dado? \n",
            "\n",
            "‚Äî¬°Joder! No lo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('sentiment-analysis')"
      ],
      "metadata": {
        "id": "NioVolm5zG5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator('this is very good'))"
      ],
      "metadata": {
        "id": "h8fwXj06zcOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}